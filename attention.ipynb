{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4 #lenght of intput : My name is Krishna\n",
    "d_k, d_v = 8,8 #size of k,v vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initializing the vectors\n",
    "q = np.random.randn(L,d_k)\n",
    "k = np.random.randn(L,d_k)\n",
    "v = np.random.randn(L,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [[ 0.71637793  2.122557    0.65611945 -2.79270637 -0.13957608 -0.42654572\n",
      "   1.33000972  0.67839715]\n",
      " [ 0.16970322  1.18782568 -0.34402368 -1.42346335 -0.17658202 -1.05895873\n",
      "   0.2645851   1.05979683]\n",
      " [ 1.4003189  -0.79588714  1.16545465  0.09623789  0.11615151 -0.09831192\n",
      "   1.820415    1.20911123]\n",
      " [-2.03504234 -0.14572441 -0.0876285   2.4955127   1.06987593  0.72536285\n",
      "   0.75068959 -0.93609233]] k: [[-0.43292757 -0.48195108 -0.22097563  0.66661556  1.3611114   0.74620559\n",
      "  -0.68710278 -0.10632383]\n",
      " [ 0.99990842  1.24237737  0.88310711  0.95270578  0.59066807 -0.7662061\n",
      "   1.06595574  0.39012535]\n",
      " [-0.56981871 -0.23800165 -0.01020816 -0.37970047 -0.19352451  1.57137181\n",
      "   0.63146991  1.69130594]\n",
      " [-0.66874421 -1.35599212  0.35196259 -0.28288742  0.99172998  0.50563394\n",
      "   1.08359416 -0.25984852]] v: [[-0.11139139 -0.0846177  -0.86453517 -1.32807291  0.5046783  -0.16446757\n",
      "  -1.143422   -0.08288812]\n",
      " [ 1.24250396  1.93482167 -0.30446374  1.00781966 -0.66378703  0.73967111\n",
      "  -0.92246277  0.13697222]\n",
      " [-0.13667546  1.17530617 -1.82175056 -0.00290617  0.94140028 -0.05652711\n",
      "   0.31618496  0.04009155]\n",
      " [-2.06640129  0.59095349 -1.28755857 -0.10637219  1.75050108 -0.54302174\n",
      "  -1.64813384 -0.5132864 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"q:\", q ,\"k:\", k, \"v:\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Attention = $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.83400888,  3.19889556,  1.48430424, -1.42548065],\n",
       "       [-2.84385259,  1.38803279,  0.49427024, -2.14182483],\n",
       "       [-1.71067669,  4.08842709,  2.36060739,  2.24961988],\n",
       "       [ 4.21538751,  0.59537599,  0.07122192,  3.30621399]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_var = np.matmul(q,k.T)/ math.sqrt(d_k)  ##scaled to minimize the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return(np.exp(x).T / np.sum(np.exp(x))).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking required in decoder but not in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask[mask == 0] = -math.inf\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00636106, 0.10888093, 0.05938581, 0.02122736],\n",
       "       [0.01285613, 0.05739851, 0.04184723, 0.01647798],\n",
       "       [0.01919134, 0.14912007, 0.08095353, 0.07783843],\n",
       "       [0.15596617, 0.04337017, 0.03603373, 0.11309156]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_fn(q,k,v,mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q,k.T)/ math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled+mask\n",
    "    attention = softmax(scaled)\n",
    "    output = np.matmul(attention,v)\n",
    "    return output,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values, attention = attention_fn(q,k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_len = 4  ##My name is Krishna\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "out_dim = 512\n",
    "x = torch.randn((batch_size,sequence_len, input_dim)) ##input to the multi head attention block\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3*out_dim)  ##q,k,v vectors all concatenated and all have the 8 attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = out_dim//num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_len, num_heads, 3*head_dim)\n",
    "qkv = qkv.permute(0,2,1,3)\n",
    "qkv.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = qkv.chunk(3,dim=-1)\n",
    "q.shape,k.shape,v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q,k.transpose(-2,-1)/math.sqrt(d_k))\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/16vzm2y9511d5n3kgmx7t_6h0000gn/T/ipykernel_99796/193267564.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3281.)\n",
      "  k.shape,k.T.shape, k.transpose(-2,-1).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([64, 4, 8, 1]),\n",
       " torch.Size([1, 8, 64, 4]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape,k.T.shape, k.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, 0., 0.],\n",
       "        [-inf, -inf, -inf, 0.],\n",
       "        [-inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.tril(mask, diagonal=1)\n",
    "mask[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   -inf,    -inf, -0.3362,  0.1631],\n",
       "        [   -inf,    -inf,    -inf,  0.7193],\n",
       "        [   -inf,    -inf,    -inf,    -inf],\n",
       "        [   -inf,    -inf,    -inf,    -inf]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled+mask)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask\n",
    "attention = F.softmax(scaled, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention,v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_attention_fn(q,k,v, mask = None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q,k.transpose(-2,-1)/math.sqrt(d_k))\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, attention = multi_attention_fn(q,k,v,mask=mask)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.5634, 0.4366],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [   nan,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan,    nan]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_dot_product(q,k,v, mask = None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q,k.transpose(-2,-1)/math.sqrt(d_k))\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention,v)\n",
    "    return values,attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model//n_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim,3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model,d_model)\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size, seq_len, input_dim = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.n_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        values,attention = attention_dot_product(q,k,v,mask)\n",
    "        values = values.reshape(batch_size,seq_len,self.n_heads*self.head_dim)\n",
    "        output = self.linear_layer(values)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "seq_len = 5\n",
    "x = torch.randn( (batch_size, seq_len, input_dim) )\n",
    "\n",
    "model = MultiHeadAttention(input_dim, n_heads, d_model)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
